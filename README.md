# MediCheck â€“ AI-Powered Symptom Checker & Health Assistant

MediCheck is a full-stack medical platform that combines intelligent symptom checking, skin condition image diagnosis, and a conversational health assistant into one seamless application. It leverages modern technologies including FastAPI, Next.js, Hugging Face Transformers, and Vision Transformers (ViT) to deliver real-time, AI-driven healthcare support.

---

## ğŸ” Features

- Symptom Checker form that predicts possible conditions based on user input.
- Image-based skin disease detection using a ViT model trained on HAM10000 and ISIC datasets.
- Conversational chatbot assistant with Retrieval-Augmented Generation (RAG) pipeline.
- Modern frontend using Next.js 14 App Router and Tailwind CSS.
- Hugging Face backend deployment for both model inference and chatbot logic.
- Secure data handling and multipart/form-data support for image + text submissions.

---

## ğŸ—‚ï¸ Project Structure

```

medical-symptom-checker/
â”‚
â”œâ”€â”€ backend/                # FastAPI backend with ML logic
â”‚   â”œâ”€â”€ main.py             # Main API entry
â”‚   â”œâ”€â”€ vitweights/         # Model directory (ViT weights, not committed)
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ frontend/               # Next.js frontend (React)
â”‚   â”œâ”€â”€ app/                # App router pages and routes
â”‚   â””â”€â”€ components/         # Reusable UI components
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md

````

---

## ğŸš€ Live Deployment

- Frontend: [https://medical-symptom-checker-five.vercel.app](https://medical-symptom-checker-five.vercel.app)
- Backend APIs: Hosted on [Hugging Face Spaces](https://huggingface.co/spaces)

---

## âš™ï¸ Getting Started Locally

### 1. Clone the Repository

```bash
git clone https://github.com/FaridaElselmy/medical-symptom-checker.git
cd medical-symptom-checker
````

---

### 2. Backend Setup (FastAPI + ViT)

> Python 3.9+ required

```bash
cd backend
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

pip install -r requirements.txt
uvicorn main:app --reload
```

Access the backend: [http://localhost:8000/docs](http://localhost:8000/docs)

Place your downloaded ViT model in:

```
backend/vitweights/
```

Update `main.py` to reflect correct path if needed.

---

### 3. Frontend Setup (Next.js 14)

> Requires Node.js v16+

```bash
cd ../frontend
npm install
npm run dev
```

Open frontend: [http://localhost:3000](http://localhost:3000)



## ğŸ“¦ Model Architecture

* **ViT-based Classifier**: Fine-tuned Vision Transformer for skin disease classification.
* **Conversational AI**: Uses HuggingFaceâ€™s `all-MiniLM-L6-v2` for semantic embeddings, Pinecone for similarity search, and LangChain with OpenAI for response generation.
* **Multipart Interoperability**: Text and image fields are combined using `FormData` and handled via FastAPIâ€™s `UploadFile` + `Form`.

---

## ğŸ” Environment Configuration

You may define variables in  `.env` (backend) if needed for secrets or endpoints like we did.



## ğŸ‘©â€ğŸ’» Authors

* [Farida Elselmy](https://github.com/FaridaElselmy)
* [Sara Ayman](https://github.com/SaraAyman-204)


